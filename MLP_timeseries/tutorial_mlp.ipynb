{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Multilayer Perceptrons (MLP)\n",
    "\n",
    "In this tutorial we will introduce an MLP for forecasting a time-series.\n",
    "\n",
    "*Remark:* This is an tutorial which intends to introduce MLPs. MLPs are usually not the best NN architectures for time series forecasting because they often do not capture the autocorrelation.\n",
    "\n",
    "The following packages are needed for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as ctp\n",
    "import nc_time_axis\n",
    "plt.style.use(\"./../plotting.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading and plotting the input data \n",
    "\n",
    "Any time-series would do the job. Here, we use the Nino3.4 index of CESM2 piControl run.\n",
    "\n",
    "We use xarray to read the .nc files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.open_dataset(\"./data/nino34_CESM2_piControl_amon.nc\")['nino34']\n",
    "_ = da.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data for our deep learning framework\n",
    "\n",
    "There are a whole zoo of deep learning libraries for python out there.\n",
    "\n",
    "![ML](img/dl_libraries.png) [Ref.](https://towardsdatascience.com/best-python-libraries-for-machine-learning-and-deep-learning-b0bd40c7e8c)\n",
    "\n",
    "Here, we use pytorch because it easily allows running your code on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch use their own datatype, pytorch.Tensors which is very similar to numpy. The following preprocessing steps are usually required: \n",
    "\n",
    "1. **Normalization**: Data should be normalized because the weights of the NN are initialized randomly around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = np.mean(da.data)\n",
    "data_std = np.std(da.data)\n",
    "da_norm = (da - data_mean) / data_std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Split data into training, validation and test set** usually by ration 70%, 20%, 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(da_norm['time']) * 0.7)\n",
    "n_val = int(len(da_norm['time']) * 0.2)\n",
    "train_data = da_norm[: n_train]\n",
    "val_data = da_norm[n_train: n_train+n_val]\n",
    "test_data = da_norm[n_train+n_val: ]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "train_data.plot(ax=ax, label='training data')\n",
    "val_data.plot(ax=ax, label='validation data')\n",
    "test_data.plot(ax=ax, label='test data')\n",
    "_ = ax.legend(bbox_to_anchor=(1,1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Convert to torch.Tensor**: It is convinient to define a Dataset class so that we can use the Dataloader objects predefined by pytorch. \n",
    "\n",
    "To process a datapoints in parallel pytorch offers a ```Dataloader```. The Dataloader allows to process a set of datapoints, i.e. minibatch at the same time. The size of the minibatch depends on the number of datapoints we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeries(Dataset):\n",
    "    def __init__(self, dataarray, n_input, lag):\n",
    "        self.n_input = n_input\n",
    "        self.lag = lag\n",
    "        self.dataarray = dataarray\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataarray['time']) - (self.n_input + self.lag)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        input = self.dataarray.isel(time=slice(idx, idx+self.n_input))\n",
    "        target = self.dataarray.isel(time=idx+self.n_input+self.lag-1)\n",
    "\n",
    "        input = torch.from_numpy(input.data).float()\n",
    "        target = torch.from_numpy(target.data[np.newaxis]).float()\n",
    "\n",
    "        label = {\n",
    "            'idx_input': torch.arange(idx, idx+self.n_input),\n",
    "            'idx_target': idx+self.n_input+self.lag-1\n",
    "        }\n",
    "\n",
    "        return input, target, label   \n",
    "        \n",
    "# Convert to torch.Dataset\n",
    "n_input = 6\n",
    "lag = 1\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TimeSeries(train_data, n_input, lag)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TimeSeries(val_data, n_input, lag)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at our pytoch Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample datapoint\n",
    "input, target, l = train_dataset[10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(l['idx_input'], input, 'bo-', label='input')\n",
    "ax.plot(l['idx_target'], target, 'gx', label='target')\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Nino3.4 (normalized)\")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Multilayer Perceptron\n",
    "\n",
    "\n",
    "**Perceptron**:\n",
    "\n",
    "<img src=\"img/perceptron.png\" width=\"500\"> [Ref.](https://commons.wikimedia.org/wiki/File:Perceptron_moj.png)\n",
    "\n",
    "The building block of a MLP is the perceptron which takes input vector $\\mathbf{x} = (x_1, \\ldots, x_n)$. \\\n",
    "The input is multiplied by the weights $\\mathbf{w} = (w_1, \\ldots, w_n)$ and are added together with a bias $w_0$. \\\n",
    "The sum is passed through a non-linear function $f$, called the activation function:\n",
    "\n",
    "$y = f\\left( \\sum_n x_n * w_n + w_0 \\right)$\n",
    "\n",
    "The most common used activation functions are ReLU, Sigmoid, tanh, ...\n",
    "\n",
    "\n",
    "**Multilayer Perceptron**:\n",
    "\n",
    "Combining perceptrons into a neural network by taking the output of the first layer as an input the the next layer.\n",
    "\n",
    "<img src=\"img/nn.png\" width=\"500\"> [Ref.](http://alexlenail.me/NN-SVG/index.html)\n",
    "\n",
    "\n",
    "### 3.1. Define an MLP\n",
    "\n",
    "We have to make the following choices when building an MLP:\n",
    "\n",
    "1. **Number of hidden layers**\n",
    "2. **Dimension of the hidden layers**\n",
    "3. **Activation function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = n_input\n",
    "output_dim = 1\n",
    "hidden_dim = 32\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(input_dim, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, output_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass f(x)\n",
    "x, t, l = train_dataset[5]\n",
    "out = mlp(x)\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Target: {t}\")\n",
    "print(f\"Prediction: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many parameters does our model have?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_p = n_{in}*n_{hid} + n_{hid} + n_{hid}*n_{out} + n_{out}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    print('Number weights:', (input_dim*hidden_dim + hidden_dim + hidden_dim*output_dim + output_dim))\n",
    "    \n",
    "    print(\"Number of trainable parameters of our model:\",\n",
    "          sum(p.numel() for p in mlp.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training the model\n",
    "\n",
    "We train the NN by computing the loss for each minibatch and update the gradients using backpropagation. We iterate $N$ epochs over the whole training data.\n",
    "\n",
    "We have to set the following hyperparameters for training:\n",
    "\n",
    "1. **Loss function**: We choose the mean-square error function because the data are Gaussian distributed:\n",
    "\n",
    "    $L(\\mathbf{w})=\\frac{1}{n} \\sum_{i=1}^{n}\\left( \\mathbf{x}^{(i)} - f_{\\mathbf{w}}\\left(\\mathbf{x}^{(i)}\\right)\\right)^{2}$\n",
    "\n",
    "2. **Optimizer**: Optimization algorithm to find the minimum of the loss function wrt. to the weights $\\mathbf{w}$, e.g. Adam, SGD, BFGS, ...\n",
    "\n",
    "3. **Learning rate $\\alpha$**: The learning rate determines the update of the weights for the respective optimizer\n",
    "    \n",
    "    $\\mathbf{w_{i+1}} = \\mathbf{w_i} - \\alpha \\cdot \\frac{\\partial L }{\\partial\\mathbf{w_i}}$\n",
    "\n",
    "4. **Epochs**: The number of epochs specifies how often we iterate over the training dataset. We choose the number of epochs such that the validation loss is converged. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "loss_fn = F.mse_loss\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over the training data and validation data for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(mlp, loss_fn, optimizer, train_loader,  val_loader, device='cpu'):\n",
    "    \"\"\"Training function of model for one epoch.\n",
    "    \"\"\"\n",
    "    mlp = mlp.to(device)\n",
    "\n",
    "    # Validation\n",
    "    # =============================================\n",
    "    mlp.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    # For validation no gradients are computed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            input, target, _ = data\n",
    "\n",
    "            # Forward pass\n",
    "            pred = mlp(input.to(device)) \n",
    "\n",
    "            # loss function\n",
    "            loss = loss_fn(target.to(device), pred)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    mean_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    \n",
    "    # Training \n",
    "    # =============================================\n",
    "    # set model into training mode\n",
    "    mlp.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Set gradients to zero in the beginning of each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input, target, _ = data\n",
    "\n",
    "        # Forward pass\n",
    "        pred = mlp(input.to(device)) \n",
    "\n",
    "        # loss function\n",
    "        loss = loss_fn(target.to(device), pred)\n",
    "\n",
    "        # backward prop and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    mean_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    return mean_train_loss, mean_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    train_epoch_loss, val_epoch_loss = training(\n",
    "        mlp, loss_fn, optimizer, train_dataloader, val_dataloader, device=device\n",
    "    )\n",
    "    print(f\"Epoch {epoch+1} of {epochs}, train. loss: {train_epoch_loss:.4f}, val. loss: {val_epoch_loss:.4f}\")\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "## 4.1. Convergence of loss\n",
    "\n",
    "The first evaluation of the training is by plotting the loss on the training data and on the validation data for each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss, label='training')\n",
    "ax.plot(val_loss, label='validation')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Visualizing prediction\n",
    "\n",
    "We take a random example and look at the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random datapoint from validation set\n",
    "input, target, l = val_dataset[np.random.randint(len(val_dataset))]\n",
    "pred = mlp(input.to(device)) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(l['idx_input'], input, 'bo-', label='input')\n",
    "ax.plot(l['idx_target'], target, 'gx', label='target')\n",
    "ax.plot(l['idx_target'], pred.cpu().detach().numpy(), 'rs', label='prediction')\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Nino3.4 (normalized)\")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred_storer = {'time':[], 'x':[], 'mse':[]}\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        input, target, l = data\n",
    "        # Forward pass\n",
    "        pred = mlp(input.to(device)) \n",
    "        pred_storer['x'].append(pred.cpu().numpy().flatten())\n",
    "        pred_storer['time'].append(val_data['time'][l['idx_target']])\n",
    "        pred_storer['mse'].append(\n",
    "            loss_fn(pred, target.to(device), reduction='none').cpu().numpy().flatten()\n",
    "        )\n",
    "\n",
    "prediction = xr.DataArray(data=np.concatenate(pred_storer['x']),\n",
    "                          coords={'time': np.concatenate(pred_storer['time'])}).sortby('time')\n",
    "mse = xr.DataArray(data=np.concatenate(pred_storer['mse']),\n",
    "                          coords={'time': np.concatenate(pred_storer['time'])}).sortby('time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction\n",
    "fig, axs = plt.subplots(2,1, sharex=True, figsize=(10,7))\n",
    "axs[0].plot(val_data['time'], val_data.data, label='target')\n",
    "axs[0].plot(prediction['time'], prediction.data, label='prediction')\n",
    "axs[0].set_ylabel('Nino3.4 (normalized)')\n",
    "axs[0].legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "\n",
    "axs[1].bar(mse['time'].data, mse.data, width=3e2)\n",
    "axs[1].set_ylabel('MSE')\n",
    "_ = axs[1].set_xlabel('time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Apply the MLP autoregressively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random datapoint from validation set\n",
    "input, target, l = val_dataset[np.random.randint(len(val_dataset))]\n",
    "x_ar = input\n",
    "n_rollout = 6\n",
    "for i in range(n_rollout):\n",
    "    pred = mlp(x_ar[-input_dim:].to(device)) \n",
    "    x_ar = torch.concat([x_ar, pred.detach().cpu()])\n",
    "\n",
    "ids = np.arange(l['idx_input'][0], l['idx_input'][-1]+n_rollout+1)\n",
    "x_ar = xr.DataArray(data=x_ar, coords={'time': val_data['time'][ids]})\n",
    "\n",
    "# Plotting rollout\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ids, val_data.isel(time=ids).data, 'bo-', label='input')\n",
    "ax.plot(ids, x_ar.data, 'rs-', label='prediction')\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Nino3.4 (normalized)\")\n",
    "ax.legend()\n",
    "_ = ax.axvline(ids[input_dim-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of the validation dataset using the MLP autoregressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rollout = 6\n",
    "with torch.no_grad():\n",
    "    pred_storer = {'time':[], 'x':[], 'mse':[]}\n",
    "    for i, data in enumerate(val_dataset):\n",
    "        input, target, l = data\n",
    "\n",
    "        # Forward pass\n",
    "        if l['idx_target']+n_rollout >= len(val_data['time']):\n",
    "            continue\n",
    "        \n",
    "        x_ar = input\n",
    "        for i in range(n_rollout):\n",
    "            pred = mlp(x_ar[-input_dim:].to(device)) \n",
    "            x_ar = torch.concat([x_ar, pred.detach().cpu()])\n",
    "\n",
    "        pred_storer['x'].append(pred.cpu().numpy())\n",
    "        pred_storer['time'].append(val_data['time'][l['idx_target']+n_rollout])\n",
    "        pred_storer['mse'].append(\n",
    "            loss_fn(pred, torch.from_numpy(val_data.isel(time=l['idx_target']+n_rollout).data.flatten()).to(device),\n",
    "                    reduction='none').cpu().numpy()\n",
    "        )\n",
    "\n",
    "prediction = xr.DataArray(data=np.array(pred_storer['x']).flatten(),\n",
    "                          coords={'time': xr.concat(pred_storer['time'], dim='time')}).sortby('time')\n",
    "mse = xr.DataArray(data=np.array(pred_storer['mse']).flatten(),\n",
    "                   coords={'time': xr.concat(pred_storer['time'], dim='time')}).sortby('time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction\n",
    "fig, axs = plt.subplots(2,1, sharex=True, figsize=(10,7))\n",
    "axs[0].plot(val_data['time'], val_data.data, label='target')\n",
    "axs[0].plot(prediction['time'], prediction.data, label='prediction')\n",
    "axs[0].set_ylabel('Nino3.4 (normalized)')\n",
    "axs[0].legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "\n",
    "axs[1].bar(mse['time'].data, mse.data, width=3e2)\n",
    "axs[1].set_ylabel('MSE')\n",
    "_ = axs[1].set_xlabel('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Typical analysis for improvements\n",
    "\n",
    "1. Increase the network until overfitting is reached\n",
    "\n",
    "2. For non-Gaussian time-series use other loss functions or bin the values for a classification task\n",
    "\n",
    "3. Use other network architectures, e.g. LSTMs for time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "\n",
    "1. [Bishop's book](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "\n",
    "2. [ML for Beginners](https://victorzhou.com/blog/intro-to-neural-networks/)\n",
    "\n",
    "3. [Introduction to NN](http://mt-class.org/jhu/slides/lecture-nn-intro.pdf)\n",
    "\n",
    "...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tutorialEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "027b997fc732148802607081a3b5c23a4e1499a6c92f8c448bf4f4f8f480623c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
